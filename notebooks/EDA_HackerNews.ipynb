{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hacker News Upvote Prediction: Enhanced EDA\n",
        "\n",
        "This notebook explores the Hacker News dataset with both **post-level** and **user-level** features to build a more robust prediction model.\n",
        "\n",
        "## Approach\n",
        "1. Load pre-extracted data (items and users)\n",
        "2. Explore post-level features (title, score, time, etc.)\n",
        "3. Explore user-level features (karma, account age, etc.)\n",
        "4. Analyze relationships between user attributes and post scores\n",
        "5. Identify key features for our prediction model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# For better visualization\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "\n",
        "print('Libraries imported successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Extracted Data\n",
        "\n",
        "We'll load the data that was previously extracted from the Hacker News database and stored as parquet files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths\n",
        "ITEMS_PATH = '../data/raw/items_100k.parquet'\n",
        "USERS_PATH = '../data/raw/users_100k.parquet'\n",
        "MERGED_PATH = '../data/raw/items_users_merged_100k.parquet'\n",
        "\n",
        "# Check if the files exist\n",
        "files_exist = all(os.path.exists(path) for path in [ITEMS_PATH, USERS_PATH, MERGED_PATH])\n",
        "\n",
        "if files_exist:\n",
        "    # Load the pre-extracted data\n",
        "    df_items = pd.read_parquet(ITEMS_PATH)\n",
        "    df_users = pd.read_parquet(USERS_PATH)\n",
        "    df_merged = pd.read_parquet(MERGED_PATH)\n",
        "    print(f\"Loaded items: {len(df_items)} rows\")\n",
        "    print(f\"Loaded users: {len(df_users)} rows\")\n",
        "    print(f\"Loaded merged dataset: {len(df_merged)} rows\")\n",
        "else:\n",
        "    print(\"Data files not found. Please run the data extraction script first.\")\n",
        "    print(\"You can run this from the command line:\")\n",
        "    print(\"python ../src/utils/data_extraction.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Explore Post-Level Features\n",
        "\n",
        "Let's first look at the structure and basic statistics of the items dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic info about the items dataset\n",
        "print(\"Items dataset columns:\")\n",
        "print(df_items.columns.tolist())\n",
        "\n",
        "# Display a few rows\n",
        "print(\"\\nSample items:\")\n",
        "df_items.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics for items\n",
        "df_items.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_counts = df_items.isnull().sum()\n",
        "missing_percent = (missing_counts / len(df_items)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_counts,\n",
        "    'Missing Percent': missing_percent\n",
        "})\n",
        "\n",
        "print(\"Missing values in items dataset:\")\n",
        "missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze Score Distribution\n",
        "\n",
        "Let's examine the distribution of scores (upvotes) which is our target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(df_items['score'], bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Hacker News Scores')\n",
        "plt.xlabel('Score (Upvotes)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Look at score distribution stats\n",
        "print(\"Score statistics:\")\n",
        "print(df_items['score'].describe())\n",
        "\n",
        "# Check for outliers\n",
        "print(\"\\nTop 10 highest scores:\")\n",
        "print(df_items['score'].nlargest(10))\n",
        "\n",
        "# Plot log-transformed score\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(np.log1p(df_items['score']), bins=50, color='lightgreen', edgecolor='black')\n",
        "plt.title('Distribution of Log-Transformed Hacker News Scores')\n",
        "plt.xlabel('Log(Score + 1)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze Titles\n",
        "\n",
        "Let's look at title characteristics and their relationship with scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add title length features\n",
        "df_items['title_length'] = df_items['title'].apply(lambda x: len(str(x)))\n",
        "df_items['title_word_count'] = df_items['title'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Plot title length distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "axes[0].hist(df_items['title_length'], bins=50, color='salmon', edgecolor='black')\n",
        "axes[0].set_title('Distribution of Title Length (Characters)')\n",
        "axes[0].set_xlabel('Title Length (Characters)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].hist(df_items['title_word_count'], bins=30, color='lightblue', edgecolor='black')\n",
        "axes[1].set_title('Distribution of Title Word Count')\n",
        "axes[1].set_xlabel('Title Word Count')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Relationship between title length and score\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(df_items['title_word_count'], df_items['score'], alpha=0.3, color='blue')\n",
        "plt.title('Score vs. Title Word Count')\n",
        "plt.xlabel('Title Word Count')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze Temporal Patterns\n",
        "\n",
        "Let's look at how scores vary over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add time-based features\n",
        "df_items['year'] = df_items['time'].dt.year\n",
        "df_items['month'] = df_items['time'].dt.month\n",
        "df_items['day_of_week'] = df_items['time'].dt.dayofweek\n",
        "df_items['hour'] = df_items['time'].dt.hour\n",
        "\n",
        "# Score by year\n",
        "yearly_scores = df_items.groupby('year')['score'].agg(['mean', 'median', 'count'])\n",
        "print(\"Scores by year:\")\n",
        "yearly_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot yearly trends\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(yearly_scores.index, yearly_scores['mean'], marker='o', linewidth=2, label='Mean Score')\n",
        "plt.plot(yearly_scores.index, yearly_scores['median'], marker='s', linewidth=2, label='Median Score')\n",
        "plt.title('Score Trends by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.xticks(yearly_scores.index)\n",
        "plt.show()\n",
        "\n",
        "# Post counts by year (to see dataset distribution)\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(yearly_scores.index, yearly_scores['count'], color='skyblue')\n",
        "plt.title('Number of Posts by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Posts')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.xticks(yearly_scores.index)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score by hour of day\n",
        "hourly_scores = df_items.groupby('hour')['score'].agg(['mean', 'median', 'count'])\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(hourly_scores.index, hourly_scores['mean'], marker='o', linewidth=2, label='Mean Score')\n",
        "plt.plot(hourly_scores.index, hourly_scores['median'], marker='s', linewidth=2, label='Median Score')\n",
        "plt.title('Score by Hour of Day (UTC)')\n",
        "plt.xlabel('Hour of Day (UTC)')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.xticks(range(0, 24, 2))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze URLs/Domains\n",
        "\n",
        "Let's look at which domains tend to get more upvotes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract domain from URL\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def extract_domain(url):\n",
        "    if not isinstance(url, str):\n",
        "        return None\n",
        "    try:\n",
        "        domain = urlparse(url).netloc\n",
        "        # Remove www. prefix if present\n",
        "        domain = re.sub(r'^www\\.', '', domain)\n",
        "        return domain if domain else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "df_items['domain'] = df_items['url'].apply(extract_domain)\n",
        "\n",
        "# Count of posts by domain\n",
        "domain_counts = df_items['domain'].value_counts()\n",
        "print(f\"Number of unique domains: {len(domain_counts)}\")\n",
        "print(\"\\nTop 20 domains by post count:\")\n",
        "print(domain_counts.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average score by domain (for domains with at least 20 posts)\n",
        "min_posts = 20\n",
        "domain_stats = df_items.groupby('domain')['score'].agg(['mean', 'median', 'count'])\n",
        "domain_stats = domain_stats[domain_stats['count'] >= min_posts].sort_values('mean', ascending=False)\n",
        "\n",
        "print(f\"Top 20 domains by average score (minimum {min_posts} posts):\")\n",
        "domain_stats.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot top domains by average score\n",
        "top_domains = domain_stats.head(15).index\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.barh(top_domains, domain_stats.loc[top_domains, 'mean'], color='skyblue')\n",
        "plt.title(f'Top 15 Domains by Average Score (min {min_posts} posts)')\n",
        "plt.xlabel('Average Score')\n",
        "plt.ylabel('Domain')\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Explore User-Level Features\n",
        "\n",
        "Now let's analyze the user dataset and see how user attributes relate to post scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic info about the users dataset\n",
        "print(\"Users dataset columns:\")\n",
        "print(df_users.columns.tolist())\n",
        "\n",
        "# Display a few rows\n",
        "print(\"\\nSample users:\")\n",
        "df_users.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics for users\n",
        "df_users.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_counts = df_users.isnull().sum()\n",
        "missing_percent = (missing_counts / len(df_users)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_counts,\n",
        "    'Missing Percent': missing_percent\n",
        "})\n",
        "\n",
        "print(\"Missing values in users dataset:\")\n",
        "missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze User Karma Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Karma distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(df_users['karma'], bins=50, color='lightgreen', edgecolor='black')\n",
        "plt.title('Distribution of User Karma')\n",
        "plt.xlabel('Karma')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Log-transformed karma distribution (to handle skew)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(np.log1p(df_users['karma']), bins=50, color='coral', edgecolor='black')\n",
        "plt.title('Distribution of Log-Transformed User Karma')\n",
        "plt.xlabel('Log(Karma + 1)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze User Account Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate account age (in days from account creation to now)\n",
        "reference_date = pd.Timestamp.now().normalize()  # Current date at midnight\n",
        "df_users['account_age_days'] = (reference_date - df_users['created']).dt.days\n",
        "\n",
        "# Calculate account age in years for better visualization\n",
        "df_users['account_age_years'] = df_users['account_age_days'] / 365.25\n",
        "\n",
        "# Account age distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(df_users['account_age_years'], bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of User Account Age')\n",
        "plt.xlabel('Account Age (Years)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Joint Analysis of Post and User Features\n",
        "\n",
        "Now let's look at how user attributes relate to post scores using the merged dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First check if we have the merged dataset\n",
        "print(\"Merged dataset columns:\")\n",
        "print(df_merged.columns.tolist())\n",
        "print(f\"\\nMerged dataset shape: {df_merged.shape}\")\n",
        "\n",
        "# Let's see how many posts have user data\n",
        "user_data_count = df_merged['karma'].notna().sum()\n",
        "print(f\"\\nPosts with user data: {user_data_count} ({user_data_count/len(df_merged)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In the merged dataset, add account age at post time\n",
        "df_merged['post_account_age_days'] = (df_merged['time'] - df_merged['created']).dt.days\n",
        "\n",
        "# Filter out rows with negative account age (impossible, would be data error)\n",
        "df_merged = df_merged[df_merged['post_account_age_days'] >= 0]\n",
        "\n",
        "# Calculate account age in years for better visualization\n",
        "df_merged['post_account_age_years'] = df_merged['post_account_age_days'] / 365.25\n",
        "\n",
        "# Analyze relationship between karma and score\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(np.log1p(df_merged['karma']), \n",
        "            np.log1p(df_merged['score']), \n",
        "            alpha=0.3, \n",
        "            color='blue')\n",
        "plt.title('Log Score vs. Log Karma')\n",
        "plt.xlabel('Log(Karma + 1)')\n",
        "plt.ylabel('Log(Score + 1)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze relationship between account age and score\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(df_merged['post_account_age_years'], \n",
        "            np.log1p(df_merged['score']), \n",
        "            alpha=0.3, \n",
        "            color='green')\n",
        "plt.title('Log Score vs. Account Age at Post Time')\n",
        "plt.xlabel('Account Age at Post Time (Years)')\n",
        "plt.ylabel('Log(Score + 1)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group users by karma buckets and see average score\n",
        "df_merged['karma_bucket'] = pd.qcut(df_merged['karma'], 10, duplicates='drop')\n",
        "karma_score = df_merged.groupby('karma_bucket')['score'].agg(['mean', 'median', 'count']).reset_index()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(range(len(karma_score)), karma_score['mean'], marker='o', linewidth=2, label='Mean Score')\n",
        "plt.plot(range(len(karma_score)), karma_score['median'], marker='s', linewidth=2, label='Median Score')\n",
        "plt.title('Score by User Karma Bucket')\n",
        "plt.xlabel('Karma Bucket (Low to High)')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.xticks(range(len(karma_score)), [str(bucket) for bucket in karma_score['karma_bucket']], rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group users by account age and see average score\n",
        "df_merged['age_bucket'] = pd.qcut(df_merged['post_account_age_years'], 10, duplicates='drop')\n",
        "age_score = df_merged.groupby('age_bucket')['score'].agg(['mean', 'median', 'count']).reset_index()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(range(len(age_score)), age_score['mean'], marker='o', linewidth=2, label='Mean Score')\n",
        "plt.plot(range(len(age_score)), age_score['median'], marker='s', linewidth=2, label='Median Score')\n",
        "plt.title('Score by User Account Age Bucket')\n",
        "plt.xlabel('Account Age Bucket (Young to Old)')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.xticks(range(len(age_score)), [str(bucket) for bucket in age_score['age_bucket']], rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Calculate Feature Correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate log transformations for highly skewed variables\n",
        "df_merged['log_score'] = np.log1p(df_merged['score'])\n",
        "df_merged['log_karma'] = np.log1p(df_merged['karma'])\n",
        "\n",
        "# Select numeric features for correlation analysis\n",
        "numeric_features = ['score', 'log_score', 'title_length', 'title_word_count', \n",
        "                    'karma', 'log_karma', 'post_account_age_days', 'year', \n",
        "                    'month', 'day_of_week', 'hour']\n",
        "\n",
        "# Filter columns that exist in the dataframe\n",
        "valid_features = [col for col in numeric_features if col in df_merged.columns]\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = df_merged[valid_features].corr()\n",
        "\n",
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Key Insights and Next Steps\n",
        "\n",
        "### Summary of Findings\n",
        "\n",
        "1. **Score Distribution**\n",
        "   - The score distribution is heavily right-skewed\n",
        "   - Log transformation makes it more amenable to modeling\n",
        "\n",
        "2. **Post Features**\n",
        "   - Title length shows [relationship with score]\n",
        "   - Posting time (hour of day, day of week) impacts scores\n",
        "   - Certain domains consistently receive higher scores\n",
        "\n",
        "3. **User Features**\n",
        "   - User karma has [relationship with post score]\n",
        "   - Account age at post time shows [relationship with score]\n",
        "\n",
        "### Feature Engineering Ideas\n",
        "\n",
        "1. **Post-level features**\n",
        "   - Title embeddings from Word2Vec\n",
        "   - Title characteristics (length, capitalization, etc.)\n",
        "   - Domain category or domain embedding\n",
        "   - Time-based features (hour, day of week, month)\n",
        "\n",
        "2. **User-level features**\n",
        "   - Log-transformed user karma\n",
        "   - Account age at post time\n",
        "   - User's post frequency\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Preprocess Data**\n",
        "   - Apply log transformation to score and karma\n",
        "   - Handle missing values appropriately\n",
        "   - Encode categorical variables\n",
        "\n",
        "2. **Word2Vec Pipeline**\n",
        "   - Pre-train on Wikipedia corpus\n",
        "   - Fine-tune on Hacker News titles\n",
        "   - Extract title embeddings\n",
        "\n",
        "3. **Feature Fusion**\n",
        "   - Combine title embeddings, user features, and other attributes\n",
        "   - Train regression model for score prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering: Justification and Strategy\n",
        "\n",
        "A robust prediction model for Hacker News upvotes requires carefully selected features. Below, we justify each candidate feature based on EDA findings and domain knowledge.\n",
        "\n",
        "## 1. Title Features\n",
        "- **Title Text (Embeddings):** The content of the title is the most direct signal of a post's topic and appeal. Word2Vec or transformer embeddings can capture semantic meaning.\n",
        "- **Title Length (Chars/Words):** While the distribution is fairly normal, outlier titles (very short/long) may affect engagement. Including length as a feature can help the model learn such effects.\n",
        "- **Keyword Flags:** Phrases like 'Show HN', 'Ask HN', or tech terms (e.g., 'AI', 'Python') are associated with different score distributions. Including binary flags for these can improve predictions.\n",
        "\n",
        "## 2. Author/User Features\n",
        "- **User Karma (log-transformed):** Higher karma users tend to get more upvotes, but the relationship is non-linear. Log transformation normalizes the distribution.\n",
        "- **Account Age at Post Time:** Older accounts may have more trust or visibility.\n",
        "- **Author ID (Categorical):** For prolific users, author identity can be predictive. For rare users, grouping as 'Other' avoids overfitting.\n",
        "\n",
        "## 3. Engagement Features\n",
        "- **Descendants (Comment Count, log-transformed):** Strongly correlated with score (correlation ~0.87 after cleaning and log transform). Indicates engagement.\n",
        "\n",
        "## 4. URL/Domain Features\n",
        "- **Is Self-Post:** Self-posts (no URL) have higher median scores than external links.\n",
        "- **Domain (Categorical):** Certain domains (e.g., arstechnica.com, github.com) are associated with higher or lower typical scores. Grouping rare domains as 'Other' is recommended.\n",
        "\n",
        "## 5. Temporal Features\n",
        "- **Hour of Day / Day of Week:** Posts during weekends and off-peak hours have higher median scores.\n",
        "- **Year/Month:** Can capture long-term trends or seasonality.\n",
        "\n",
        "## 6. Status Features\n",
        "- **Dead Flag:** Posts marked as 'dead' are rare in cleaned data, but if present, should be included as a binary feature.\n",
        "\n",
        "## Feature Selection Summary\n",
        "- **Include:** Title embeddings, title length, keyword flags, log(karma), account age, author ID (for frequent posters), log(descendants), is_self_post, domain, hour, dayofweek.\n",
        "- **Exclude:** Raw title/URL length (no correlation), post count per author (no predictive value).\n",
        "\n",
        "## Next Steps\n",
        "- **Preprocessing:** Apply log transforms, handle missing values, encode categoricals.\n",
        "- **Modeling:** Use a regression model (e.g., LightGBM, Ridge, or neural net) with the above features.\n",
        "- **Evaluation:** Focus on log(score+1) as the target due to skewness.\n",
        "\n",
        "---\n",
        "**References:**\n",
        "- See the EDA above for empirical evidence supporting each feature.\n",
        "- For more, see the extended EDA in `eda.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion & Recommendations\n",
        "\n",
        "- The Hacker News dataset exhibits strong non-linearities and skewed distributions.\n",
        "- Feature engineering, especially log transforms and categorical encoding, is essential.\n",
        "- Engagement (comments), timing, and content all matter.\n",
        "- Next, proceed to model training with the engineered features."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
