{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca38b7b",
   "metadata": {},
   "source": [
    "# Hacker News Titles Preprocessing\n",
    "\n",
    "This notebook replicates the original script for fetching, preprocessing, tokenizing, and uploading Hacker News titles. Each section is organized into separate cells for clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73580b72",
   "metadata": {},
   "source": [
    "## 1. Imports and NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02118f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pickle\n",
    "import psycopg2\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171b4f9",
   "metadata": {},
   "source": [
    "## 2. Connect to Postgres and Fetch Hacker News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the connection string as needed\n",
    "conn = psycopg2.connect(\"postgres://sy91dhb:g5t49ao@178.156.142.230:5432/hd64m1ki\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SELECT title, score FROM \"hacker_news\".\"items\" \n",
    "               WHERE title IS NOT NULL AND score IS NOT NULL;\"\"\")\n",
    "data = cur.fetchall()\n",
    "titles = [row[0] for row in data]\n",
    "scores = [row[1] for row in data]\n",
    "conn.close()\n",
    "\n",
    "print(f\"Loaded {len(titles)} titles from Hacker News\")\n",
    "print(f\"Sample titles: {titles[:3]}\")\n",
    "print(f\"Sample scores: {scores[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9345eff0",
   "metadata": {},
   "source": [
    "## 3. Preprocess Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(text: str) -> list[str]:\n",
    "#     if not isinstance(text, str):\n",
    "#         return []\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
    "#     text = text.replace('-', ' ')\n",
    "#     words = word_tokenize(text)\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     words = [word for word in words if word not in stop_words]\n",
    "#     stats = collections.Counter(words)\n",
    "#     words = [word for word in words if stats[word] > 5]\n",
    "#     return words\n",
    "\n",
    "# # Process all titles\n",
    "# all_tokens = []\n",
    "# for title in titles:\n",
    "#     tokens = preprocess(title)\n",
    "#     all_tokens.extend(tokens)\n",
    "\n",
    "# corpus = all_tokens\n",
    "# print(f\"Total tokens: {len(corpus)}\")\n",
    "# print(f\"Sample tokens: {corpus[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baea2ca6",
   "metadata": {},
   "source": [
    "## 4. Save Corpus and Title Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save corpus and title tokens\n",
    "# with open('corpus.pkl', 'wb') as f:\n",
    "#     pickle.dump(corpus, f)\n",
    "\n",
    "# title_tokens = [preprocess(title) for title in titles]\n",
    "# with open('title_tokens.pkl', 'wb') as f:\n",
    "#     pickle.dump(title_tokens, f)\n",
    "# with open('scores.pkl', 'wb') as f:\n",
    "#     pickle.dump(scores, f)\n",
    "\n",
    "# print(\"Saved corpus.pkl, title_tokens.pkl, scores.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fe2a4",
   "metadata": {},
   "source": [
    "## 5. Create Lookup Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f31886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_lookup_tables(words: list[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "#     word_counts = collections.Counter(words)\n",
    "#     vocab = sorted(word_counts, key=lambda k: word_counts.get(k), reverse=True)\n",
    "#     int_to_vocab = {ii+1: word for ii, word in enumerate(vocab)}\n",
    "#     int_to_vocab[0] = '<PAD>'\n",
    "#     vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "#     return vocab_to_int, int_to_vocab\n",
    "\n",
    "# # Create mappings\n",
    "# words_to_ids, ids_to_words = create_lookup_tables(corpus)\n",
    "# tokens = [words_to_ids[word] for word in corpus]\n",
    "# print(f\"Vocabulary size: {len(words_to_ids)}\")\n",
    "# print(f\"Sample token IDs: {tokens[:10]}\")\n",
    "\n",
    "# # Save mappings\n",
    "# with open('words_to_ids.pkl', 'wb') as f:\n",
    "#     pickle.dump(words_to_ids, f)\n",
    "# with open('ids_to_words.pkl', 'wb') as f:\n",
    "#     pickle.dump(ids_to_words, f)\n",
    "\n",
    "# print(\"Saved words_to_ids.pkl, ids_to_words.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f8ac44",
   "metadata": {},
   "source": [
    "## 6. Tokenize Titles to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ba2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_token_ids = []\n",
    "# for tokens in title_tokens:\n",
    "#     title_ids = [words_to_ids.get(word, 0) for word in tokens]\n",
    "#     title_token_ids.append(title_ids)\n",
    "\n",
    "# with open('title_token_ids.pkl', 'wb') as f:\n",
    "#     pickle.dump(title_token_ids, f)\n",
    "\n",
    "# print(\"Saved title_token_ids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4646a30",
   "metadata": {},
   "source": [
    "## 7. Upload Files to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f144c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
